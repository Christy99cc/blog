---
title: "【ML】降维"
date: 2020-11-17T16:00:00Z
tags:
- 机器学习
categories: ''

---

> 吴恩达机器学习
> 无监督学习

## 数据压缩
❓降维是什么
### 二维降到一维
有一组数据，有两个特征x1，x2，这两个特征中，x1表示单位是厘米的长度，x2表示单位是英尺的长度。这样的话，就存在冗余。
我们可以用一个数字来代表原来的两个数字，也就是用一维的数据代表原来二维的数据。
![image](https://cdn.sparkling.land/christy/images/B7C3FA87-E292-4C8A-95BC-8C592E2F2B59.jpg)￼

### 三维降到二维
这组数据，实际上分布在一个平面上
![image](https://cdn.sparkling.land/christy/images/1E83E19B-7242-4FA0-B03D-A1E65212F15E.jpg)￼


## 数据可视化

![image](https://cdn.sparkling.land/christy/images/FA4488E0-FBD6-4E45-A614-30AAD4249F43.jpg)￼
![image](https://cdn.sparkling.land/christy/images/EDC770A2-C8DB-430A-9F8B-CCA3BE00CD99.jpg)￼
![image](https://cdn.sparkling.land/christy/images/E512FABA-CB00-4BFE-B9C1-4873FB32965F.jpg)￼


## PCA
PCA是找一个低维平面，把数据投影在上面，使这些蓝色线段的长度和最小。这个蓝色线段长度也叫做投影误差，PCA想要最小化这个距离。
![image](https://cdn.sparkling.land/christy/images/1E3B2252-AB78-49A6-B061-799F995A285B.jpg)￼

### PCA与线性回归的关系？
PCA不是线性回归！

![image](https://cdn.sparkling.land/christy/images/BDB29E77-6288-44EA-81E4-F14C1C143CC2.jpg)￼

| 线性回归（左图） | PCA（右图） |
| 最小化的是竖直距离，蓝色的线段 | 最小化的是投影到直线的距离，蓝色的线段 |
| 有一个y，要预测这个y | 都是x，没有区别对待，要用所有的这些x预测y |
￼

###  PCA算法
PCA参考下面的blog，很好理解，相应的线性代数的知识可以补充看看。
[http://blog.codinglabs.org/articles/pca-tutorial.html](http://blog.codinglabs.org/articles/pca-tutorial.html)￼

## 关于K的选择
![image](https://cdn.sparkling.land/christy/images/12ABED9F-EDB6-4E6F-A11B-0B84829E97FC.jpg)￼
![image](https://cdn.sparkling.land/christy/images/F37F805E-84C8-4D05-8570-6B32AB9239B8.jpg)￼

## PCA的应用

![image](https://cdn.sparkling.land/christy/images/56EFF92A-FA4D-4A1A-BF75-8C4BB0B5F4E1.jpg)￼
![image](https://cdn.sparkling.land/christy/images/8C166EAD-0655-49FA-9D2E-83B5FA06AC78.jpg)￼

![image](https://cdn.sparkling.land/christy/images/43919679-9B12-42A1-B7F8-C58046ED46AC.jpg)￼
PCA降维虽然可能避免过拟合，但是这不是一个好的解决方案；过拟合还是要通过＋正则化项来解决。
![image](https://cdn.sparkling.land/christy/images/D993300C-6470-4988-9DCD-F1192D1FE273.jpg)￼

