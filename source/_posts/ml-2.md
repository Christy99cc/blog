---
title: "【ML】随机初始化"
date: 2020-11-14T16:00:00Z
tags:
- 机器学习
categories: ''

---
# 随机初始化
Random Initialization
在实现梯度下降或其他算法的时候，需要为θ等参数赋初值。
**一种做法：赋全0**
![image](https://cdn.sparkling.land/christy/images/D4F38C6D-FDEA-4E15-B3E5-48525B8895A5.jpg)￼

初始化的时候，两条蓝色的θ为0，两条红色的θ为0，两条绿色的θ为0；
那么a21，a22都是以相同的输入函数来计算的；
对于神经网络中所有的训练样本，最后总能得到**a21=a22；
**也能得到**delt(a21)=delt(a22)**。****
以蓝色为例来说，代价函数对于两条蓝色的θ的偏导也相等；更新也会相等；多少次的迭代都是相等的更新。
对于绿色、红色，也是一样的。
蓝色的两条，绿色的两条，红色的两条，始终分别是相等的。这意味着在计算相同的特征，高度冗余。





另一种做法：
随机初始化，打破对称性。
![image](https://cdn.sparkling.land/christy/images/2403E2CF-F43A-49FF-BA6D-B2EAF6D4FAF6.jpg)￼

注意：这里的ε与梯度检验里的ε没有任何关系，只是恰好用了相同的字母。

训练之前，要将权重矩阵随机初始化成一个接近于0，在[-ε, ε]的值，然后进行反向传播，再进行梯度检验，最后进行梯度下降或者其他高级优化算法，来最小化代价函数J(θ)